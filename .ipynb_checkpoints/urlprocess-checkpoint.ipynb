{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f3a02d6-0dc3-45bc-9e06-f4df37efd488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./anaconda3/envs/myenv/lib/python3.10/site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in ./anaconda3/envs/myenv/lib/python3.10/site-packages (4.12.2)\n",
      "Collecting fake-useragent\n",
      "  Downloading fake_useragent-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: pillow in ./anaconda3/envs/myenv/lib/python3.10/site-packages (10.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./anaconda3/envs/myenv/lib/python3.10/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/envs/myenv/lib/python3.10/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./anaconda3/envs/myenv/lib/python3.10/site-packages (from requests) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/envs/myenv/lib/python3.10/site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./anaconda3/envs/myenv/lib/python3.10/site-packages (from beautifulsoup4) (2.5)\n",
      "Downloading fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fake-useragent\n",
      "Successfully installed fake-useragent-2.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 fake-useragent pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8e6dd89-85bb-428c-bb66-44c2e2fbc953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News Article Extraction Agent\n",
      "Enter news article URLs to extract content (or type 'quit' to exit):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter URL:   https://mothership.sg/2025/07/soviet-era-plane/\n",
      "Download images? (y/n):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing https://mothership.sg/2025/07/soviet-era-plane/...\n",
      "Extracting article from: https://mothership.sg/2025/07/soviet-era-plane/\n",
      "Error fetching page: 403 Client Error: Forbidden for url: https://mothership.sg/2025/07/soviet-era-plane/\n",
      "Processing completed in 0.11 seconds\n",
      "Error: Failed to fetch page content\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter URL:  https://www.straitstimes.com/life/disney-k-drama-delusion-faces-backlash-over-leaving-piles-of-trash-in-jeju-after-filming\n",
      "Download images? (y/n):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing https://www.straitstimes.com/life/disney-k-drama-delusion-faces-backlash-over-leaving-piles-of-trash-in-jeju-after-filming...\n",
      "Extracting article from: https://www.straitstimes.com/life/disney-k-drama-delusion-faces-backlash-over-leaving-piles-of-trash-in-jeju-after-filming\n",
      "Processing completed in 0.20 seconds\n",
      "\n",
      "==================================================\n",
      "TITLE: Disney+ K-drama Delusion faces backlash over leaving piles of trash in Jeju after filming\n",
      "==================================================\n",
      "\n",
      "METADATA:\n",
      "  Date: 2025-08-29T16:25:00+08:00\n",
      "  Description: The clip revealed discarded items ranging from butane gas canisters to branded coffee cup sleeves.  Read more at straitstimes.com. Read more at straitstimes.com.\n",
      "\n",
      "BODY (first 500 characters):\n",
      "Disney+ K-drama Delusion faces backlash over leaving piles of trash in Jeju after filmingSign up now: Get ST's newsletters delivered to your inboxDelusion stars South Korean actors Bae Suzy (middle) and Kim Seon-ho (left).PHOTO: DISNEY+Follow topic:South KoreaPublished Aug 29, 2025, 04:25 PMUpdated Aug 29, 2025, 04:25 PMSEOUL - The production team behind upcoming Disney+ drama series Delusion, starring Bae Suzy and Kim Seon-ho, has come under fire after leaving piles of trash behind at a filming...\n",
      "\n",
      "IMAGES FOUND: 3\n",
      "  1. https://www.straitstimes.com/assets/ST-logo-default-aoeSLh4S.png\n",
      "     Alt text: Straitstimes.com header logo\n",
      "  2. https://cassette.sphdigital.com.sg/image/straitstimes/43f59c3279db72f315f2cf2ab55b24907253a8f1f80fe08e549d899fce32d8f0\n",
      "     Alt text: Delusion stars South Korean actors Bae Suzy (middle) and Kim Seon-ho (left).\n",
      "  3. https://www.straitstimes.com/assets/subscribe-placeholder-sF6eULDl.png\n",
      "     Alt text: Subscribe Placeholder\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 385\u001b[0m\n\u001b[1;32m    382\u001b[0m extractor \u001b[38;5;241m=\u001b[39m NewsArticleExtractor()\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# Start interactive mode\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m \u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_user_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 372\u001b[0m, in \u001b[0;36mNewsArticleExtractor.process_user_input\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m     Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# Ask if user wants to save results to file\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m save_choice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mSave results to JSON file? (y/n): \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_choice \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    374\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/ipykernel/kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/ipykernel/kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "News Article Extraction Agent\n",
    "Extracts Header, Body, and Images from news article URLs\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "from fake_useragent import UserAgent\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "class NewsArticleExtractor:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.ua = UserAgent()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': self.ua.random,\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        })\n",
    "        \n",
    "    def is_valid_url(self, url):\n",
    "        \"\"\"Check if the URL is valid\"\"\"\n",
    "        try:\n",
    "            result = urlparse(url)\n",
    "            return all([result.scheme, result.netloc])\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def get_page_content(self, url):\n",
    "        \"\"\"Fetch the content of a web page\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching page: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_title(self, soup):\n",
    "        \"\"\"Extract the article title\"\"\"\n",
    "        # Try different meta tags and elements that might contain the title\n",
    "        selectors = [\n",
    "            'meta[property=\"og:title\"]',\n",
    "            'meta[name=\"twitter:title\"]',\n",
    "            'h1',\n",
    "            '.headline',\n",
    "            '.title',\n",
    "            '[class*=\"title\"]',\n",
    "            '[class*=\"headline\"]',\n",
    "            'title'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            elements = soup.select(selector)\n",
    "            for element in elements:\n",
    "                if selector.startswith('meta'):\n",
    "                    title = element.get('content', '').strip()\n",
    "                else:\n",
    "                    title = element.get_text().strip()\n",
    "                \n",
    "                if title and len(title) > 10 and len(title) < 200:\n",
    "                    return title\n",
    "        \n",
    "        # Fallback to the page title\n",
    "        if soup.title:\n",
    "            return soup.title.get_text().strip()\n",
    "        \n",
    "        return \"Title not found\"\n",
    "    \n",
    "    def extract_body(self, soup):\n",
    "        \"\"\"Extract the main article body text\"\"\"\n",
    "        # Try to find the main content area\n",
    "        selectors = [\n",
    "            'article',\n",
    "            '.article-body',\n",
    "            '.post-content',\n",
    "            '.entry-content',\n",
    "            '.story-content',\n",
    "            '[class*=\"content\"]',\n",
    "            '[class*=\"body\"]',\n",
    "            'main'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            elements = soup.select(selector)\n",
    "            for element in elements:\n",
    "                # Clean up the text\n",
    "                text = element.get_text()\n",
    "                text = re.sub(r'\\s+', ' ', text).strip()\n",
    "                \n",
    "                if len(text) > 100:  # Reasonable minimum length for an article\n",
    "                    return text\n",
    "        \n",
    "        # If no specific content area found, try to find paragraphs\n",
    "        paragraphs = soup.find_all('p')\n",
    "        text = ' '.join([p.get_text() for p in paragraphs])\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        if len(text) > 100:\n",
    "            return text\n",
    "        \n",
    "        return \"Body content not found or too short\"\n",
    "    \n",
    "    def extract_images(self, soup, base_url):\n",
    "        \"\"\"Extract images from the article\"\"\"\n",
    "        images = []\n",
    "        \n",
    "        # Look for images in various locations\n",
    "        img_selectors = [\n",
    "            'img',\n",
    "            'meta[property=\"og:image\"]',\n",
    "            'meta[name=\"twitter:image\"]',\n",
    "            'link[rel=\"image_src\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in img_selectors:\n",
    "            elements = soup.select(selector)\n",
    "            for element in elements:\n",
    "                if selector.startswith('meta'):\n",
    "                    img_url = element.get('content', '')\n",
    "                elif selector.startswith('link'):\n",
    "                    img_url = element.get('href', '')\n",
    "                else:\n",
    "                    img_url = element.get('src', '')\n",
    "                \n",
    "                if img_url:\n",
    "                    # Make URL absolute\n",
    "                    img_url = urljoin(base_url, img_url)\n",
    "                    \n",
    "                    # Get alt text for regular img tags\n",
    "                    alt_text = element.get('alt', '') if selector == 'img' else ''\n",
    "                    \n",
    "                    # Avoid duplicates\n",
    "                    if img_url not in [img['url'] for img in images]:\n",
    "                        images.append({\n",
    "                            'url': img_url,\n",
    "                            'alt': alt_text\n",
    "                        })\n",
    "        \n",
    "        return images\n",
    "    \n",
    "    def extract_metadata(self, soup):\n",
    "        \"\"\"Extract additional metadata from the article\"\"\"\n",
    "        metadata = {}\n",
    "        \n",
    "        # Publication date\n",
    "        date_selectors = [\n",
    "            'meta[property=\"article:published_time\"]',\n",
    "            'meta[name=\"date\"]',\n",
    "            'meta[name=\"publish_date\"]',\n",
    "            'time',\n",
    "            '[class*=\"date\"]',\n",
    "            '[class*=\"time\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in date_selectors:\n",
    "            elements = soup.select(selector)\n",
    "            for element in elements:\n",
    "                if selector.startswith('meta'):\n",
    "                    date = element.get('content', '')\n",
    "                else:\n",
    "                    date = element.get('datetime', '') or element.get_text()\n",
    "                \n",
    "                if date and 'date' not in metadata:\n",
    "                    metadata['date'] = date.strip()\n",
    "                    break\n",
    "        \n",
    "        # Author\n",
    "        author_selectors = [\n",
    "            'meta[name=\"author\"]',\n",
    "            'meta[property=\"article:author\"]',\n",
    "            '[rel=\"author\"]',\n",
    "            '.author',\n",
    "            '[class*=\"author\"]',\n",
    "            '[class*=\"byline\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in author_selectors:\n",
    "            elements = soup.select(selector)\n",
    "            for element in elements:\n",
    "                if selector.startswith('meta'):\n",
    "                    author = element.get('content', '')\n",
    "                else:\n",
    "                    author = element.get_text()\n",
    "                \n",
    "                if author and 'author' not in metadata:\n",
    "                    metadata['author'] = author.strip()\n",
    "                    break\n",
    "        \n",
    "        # Description\n",
    "        desc_selectors = [\n",
    "            'meta[property=\"og:description\"]',\n",
    "            'meta[name=\"description\"]',\n",
    "            'meta[name=\"twitter:description\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in desc_selectors:\n",
    "            elements = soup.select(selector)\n",
    "            for element in elements:\n",
    "                description = element.get('content', '')\n",
    "                if description and 'description' not in metadata:\n",
    "                    metadata['description'] = description.strip()\n",
    "                    break\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    def download_image(self, image_url, save_dir=None):\n",
    "        \"\"\"Download an image and return information about it\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(image_url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            img_data = response.content\n",
    "            img = Image.open(io.BytesIO(img_data))\n",
    "            \n",
    "            image_info = {\n",
    "                'url': image_url,\n",
    "                'format': img.format,\n",
    "                'size': len(img_data),\n",
    "                'dimensions': img.size,\n",
    "                'mode': img.mode,\n",
    "                'downloaded': True\n",
    "            }\n",
    "            \n",
    "            # Save image if directory provided\n",
    "            if save_dir:\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                filename = os.path.join(save_dir, f\"image_{int(time.time())}_{hash(image_url)}.{img.format.lower()}\")\n",
    "                with open(filename, 'wb') as f:\n",
    "                    f.write(img_data)\n",
    "                image_info['saved_path'] = filename\n",
    "            \n",
    "            return image_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading image {image_url}: {e}\")\n",
    "            return {\n",
    "                'url': image_url,\n",
    "                'error': str(e),\n",
    "                'downloaded': False\n",
    "            }\n",
    "    \n",
    "    def extract_article(self, url, download_images=False, image_save_dir=None):\n",
    "        \"\"\"\n",
    "        Extract information from a news article URL\n",
    "        \n",
    "        Args:\n",
    "            url (str): The URL of the news article\n",
    "            download_images (bool): Whether to download images\n",
    "            image_save_dir (str): Directory to save downloaded images\n",
    "            \n",
    "        Returns:\n",
    "            dict: Article information including header, body, and images\n",
    "        \"\"\"\n",
    "        if not self.is_valid_url(url):\n",
    "            return {\"error\": \"Invalid URL\"}\n",
    "        \n",
    "        print(f\"Extracting article from: {url}\")\n",
    "        \n",
    "        # Fetch the page content\n",
    "        content = self.get_page_content(url)\n",
    "        if not content:\n",
    "            return {\"error\": \"Failed to fetch page content\"}\n",
    "        \n",
    "        # Parse with BeautifulSoup\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        \n",
    "        # Extract article components\n",
    "        title = self.extract_title(soup)\n",
    "        body = self.extract_body(soup)\n",
    "        images = self.extract_images(soup, url)\n",
    "        metadata = self.extract_metadata(soup)\n",
    "        \n",
    "        # Download images if requested\n",
    "        downloaded_images = []\n",
    "        if download_images and images:\n",
    "            print(f\"Downloading {len(images)} images...\")\n",
    "            for img in images:\n",
    "                img_info = self.download_image(img['url'], image_save_dir)\n",
    "                img_info['alt'] = img.get('alt', '')\n",
    "                downloaded_images.append(img_info)\n",
    "        \n",
    "        # Prepare result\n",
    "        result = {\n",
    "            \"url\": url,\n",
    "            \"title\": title,\n",
    "            \"body\": body,\n",
    "            \"images\": downloaded_images if download_images else images,\n",
    "            \"metadata\": metadata,\n",
    "            \"success\": True\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def process_user_input(self):\n",
    "        \"\"\"Interactive mode for processing URLs\"\"\"\n",
    "        print(\"News Article Extraction Agent\")\n",
    "        print(\"Enter news article URLs to extract content (or type 'quit' to exit):\")\n",
    "        \n",
    "        while True:\n",
    "            url = input(\"\\nEnter URL: \").strip()\n",
    "            \n",
    "            if url.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"Exiting News Article Extractor.\")\n",
    "                break\n",
    "                \n",
    "            if not url:\n",
    "                print(\"Please enter a URL.\")\n",
    "                continue\n",
    "                \n",
    "            if not self.is_valid_url(url):\n",
    "                print(\"Invalid URL. Please enter a valid URL including http:// or https://\")\n",
    "                continue\n",
    "            \n",
    "            # Ask if user wants to download images\n",
    "            download_choice = input(\"Download images? (y/n): \").strip().lower()\n",
    "            download_images = download_choice in ['y', 'yes']\n",
    "            \n",
    "            image_save_dir = None\n",
    "            if download_images:\n",
    "                image_save_dir = input(\"Enter directory to save images (press Enter for current directory): \").strip()\n",
    "                if not image_save_dir:\n",
    "                    image_save_dir = \"downloaded_images\"\n",
    "            \n",
    "            # Extract article\n",
    "            print(f\"\\nProcessing {url}...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            result = self.extract_article(url, download_images, image_save_dir)\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            print(f\"Processing completed in {processing_time:.2f} seconds\")\n",
    "            \n",
    "            # Display results\n",
    "            if result.get('error'):\n",
    "                print(f\"Error: {result['error']}\")\n",
    "                continue\n",
    "                \n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(f\"TITLE: {result['title']}\")\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "            if result['metadata']:\n",
    "                print(\"\\nMETADATA:\")\n",
    "                for key, value in result['metadata'].items():\n",
    "                    print(f\"  {key.capitalize()}: {value}\")\n",
    "            \n",
    "            print(f\"\\nBODY (first 500 characters):\")\n",
    "            body_preview = result['body'][:500] + \"...\" if len(result['body']) > 500 else result['body']\n",
    "            print(f\"{body_preview}\")\n",
    "            \n",
    "            print(f\"\\nIMAGES FOUND: {len(result['images'])}\")\n",
    "            for i, img in enumerate(result['images'], 1):\n",
    "                print(f\"  {i}. {img['url']}\")\n",
    "                if img.get('alt'):\n",
    "                    print(f\"     Alt text: {img['alt']}\")\n",
    "                if img.get('downloaded', False) and img['downloaded']:\n",
    "                    print(f\"     Downloaded: {img.get('saved_path', 'Yes')}\")\n",
    "                elif img.get('error'):\n",
    "                    print(f\"     Error: {img['error']}\")\n",
    "            \n",
    "            # Ask if user wants to save results to file\n",
    "            save_choice = input(\"\\nSave results to JSON file? (y/n): \").strip().lower()\n",
    "            if save_choice in ['y', 'yes']:\n",
    "                filename = f\"article_{int(time.time())}.json\"\n",
    "                with open(filename, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "                print(f\"Results saved to {filename}\")\n",
    "\n",
    "# Demonstration and Testing\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the extractor\n",
    "    extractor = NewsArticleExtractor()\n",
    "    \n",
    "    # Start interactive mode\n",
    "    extractor.process_user_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82db6804-ff7e-4909-94ed-d82f2e28a60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with: https://mothership.sg/2025/07/soviet-era-plane/\n",
      "Extracting article from: https://mothership.sg/2025/07/soviet-era-plane/\n",
      "403 detected, trying with cloudscraper...\n",
      "Successfully extracted: At least 46 on board Soviet-era plane killed in crash in Russia, criminal probe launched\n",
      "Body preview: News July 26, 2025, 10:09 AM At least 46 on board Soviet-era plane killed in crash in Russia, criminal probe launched...\n",
      "Enhanced News Article Extraction Agent\n",
      "Enter news article URLs to extract content (or type 'quit' to exit):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter URL:  https://www.straitstimes.com/life/disney-k-drama-delusion-faces-backlash-over-leaving-piles-of-trash-in-jeju-after-filming\n",
      "Download images? (y/n):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing https://www.straitstimes.com/life/disney-k-drama-delusion-faces-backlash-over-leaving-piles-of-trash-in-jeju-after-filming...\n",
      "Extracting article from: https://www.straitstimes.com/life/disney-k-drama-delusion-faces-backlash-over-leaving-piles-of-trash-in-jeju-after-filming\n",
      "Processing completed in 1.24 seconds\n",
      "\n",
      "==================================================\n",
      "TITLE: Disney+ K-drama Delusion faces backlash over leaving piles of trash in Jeju after filming\n",
      "==================================================\n",
      "\n",
      "METADATA:\n",
      "  Date: 2025-08-29T16:25:00+08:00\n",
      "  Description: The clip revealed discarded items ranging from butane gas canisters to branded coffee cup sleeves.  Read more at straitstimes.com. Read more at straitstimes.com.\n",
      "\n",
      "BODY (first 500 characters):\n",
      "Disney+ K-drama Delusion faces backlash over leaving piles of trash in Jeju after filmingSign up now: Get ST's newsletters delivered to your inboxDelusion stars South Korean actors Bae Suzy (middle), Kim Seon-ho (left) and director Han Jae-rim.PHOTO: DISNEY+Follow topic:South KoreaPublished Aug 29, 2025, 04:25 PMUpdated Aug 29, 2025, 04:43 PMSEOUL - The production team behind upcoming Disney+ drama series Delusion, starring Bae Suzy and Kim Seon-ho, has come under fire after leaving piles of tra...\n",
      "\n",
      "IMAGES FOUND: 3\n",
      "  1. https://www.straitstimes.com/assets/ST-logo-default-aoeSLh4S.png\n",
      "     Alt text: Straitstimes.com header logo\n",
      "  2. https://cassette.sphdigital.com.sg/image/straitstimes/43f59c3279db72f315f2cf2ab55b24907253a8f1f80fe08e549d899fce32d8f0\n",
      "     Alt text: Delusion stars South Korean actors Bae Suzy (middle) and Kim Seon-ho (left).\n",
      "  3. https://www.straitstimes.com/assets/subscribe-placeholder-sF6eULDl.png\n",
      "     Alt text: Subscribe Placeholder\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 502\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to extract content: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown error\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;66;03m# Start interactive mode\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m \u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_user_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 438\u001b[0m, in \u001b[0;36mEnhancedNewsArticleExtractor.process_user_input\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  ... and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m5\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m more images\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m# Ask if user wants to save results to file\u001b[39;00m\n\u001b[0;32m--> 438\u001b[0m save_choice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mSave results to JSON file? (y/n): \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_choice \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    440\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/ipykernel/kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/ipykernel/kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Enhanced News Article Extraction Agent\n",
    "With better handling of anti-scraping measures\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "from fake_useragent import UserAgent\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import io\n",
    "import random\n",
    "import cloudscraper  # For bypassing Cloudflare protection\n",
    "\n",
    "class EnhancedNewsArticleExtractor:\n",
    "    def __init__(self):\n",
    "        self.ua = UserAgent()\n",
    "        self.session = self._create_session()\n",
    "        self.scraper = cloudscraper.create_scraper()  # For Cloudflare protection\n",
    "        self.request_delay = 1  # Delay between requests in seconds\n",
    "        self.max_retries = 3\n",
    "        \n",
    "    def _create_session(self):\n",
    "        \"\"\"Create a requests session with realistic headers\"\"\"\n",
    "        session = requests.Session()\n",
    "        session.headers.update({\n",
    "            'User-Agent': self.ua.random,\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "            'Cache-Control': 'max-age=0',\n",
    "            'DNT': '1',\n",
    "        })\n",
    "        return session\n",
    "    \n",
    "    def _rotate_user_agent(self):\n",
    "        \"\"\"Rotate to a new user agent\"\"\"\n",
    "        self.session.headers['User-Agent'] = self.ua.random\n",
    "    \n",
    "    def is_valid_url(self, url):\n",
    "        \"\"\"Check if the URL is valid\"\"\"\n",
    "        try:\n",
    "            result = urlparse(url)\n",
    "            return all([result.scheme, result.netloc])\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def get_page_content(self, url, retry_count=0):\n",
    "        \"\"\"Fetch the content of a web page with retries and agent rotation\"\"\"\n",
    "        if retry_count >= self.max_retries:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Add delay to be more polite\n",
    "            time.sleep(self.request_delay)\n",
    "            \n",
    "            # Try with regular session first\n",
    "            response = self.session.get(url, timeout=15)\n",
    "            \n",
    "            # If we get a 403, try with cloudscraper\n",
    "            if response.status_code == 403:\n",
    "                print(\"403 detected, trying with cloudscraper...\")\n",
    "                response = self.scraper.get(url, timeout=15)\n",
    "            \n",
    "            response.raise_for_status()\n",
    "            return response.content\n",
    "            \n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if e.response.status_code == 403:\n",
    "                print(f\"403 Forbidden error (attempt {retry_count + 1}/{self.max_retries})\")\n",
    "                self._rotate_user_agent()\n",
    "                return self.get_page_content(url, retry_count + 1)\n",
    "            else:\n",
    "                print(f\"HTTP error {e.response.status_code}: {e}\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching page: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_title(self, soup):\n",
    "        \"\"\"Extract the article title\"\"\"\n",
    "        # Try different meta tags and elements that might contain the title\n",
    "        selectors = [\n",
    "            'meta[property=\"og:title\"]',\n",
    "            'meta[name=\"twitter:title\"]',\n",
    "            'h1',\n",
    "            '.headline',\n",
    "            '.title',\n",
    "            '[class*=\"title\"]',\n",
    "            '[class*=\"headline\"]',\n",
    "            'title'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            elements = soup.select(selector)\n",
    "            for element in elements:\n",
    "                if selector.startswith('meta'):\n",
    "                    title = element.get('content', '').strip()\n",
    "                else:\n",
    "                    title = element.get_text().strip()\n",
    "                \n",
    "                if title and len(title) > 10 and len(title) < 200:\n",
    "                    return title\n",
    "        \n",
    "        # Fallback to the page title\n",
    "        if soup.title:\n",
    "            return soup.title.get_text().strip()\n",
    "        \n",
    "        return \"Title not found\"\n",
    "    \n",
    "    def extract_body(self, soup):\n",
    "        \"\"\"Extract the main article body text\"\"\"\n",
    "        # Remove unwanted elements\n",
    "        for unwanted in soup(['script', 'style', 'nav', 'footer', 'aside', 'form']):\n",
    "            unwanted.decompose()\n",
    "        \n",
    "        # Try to find the main content area\n",
    "        selectors = [\n",
    "            'article',\n",
    "            '.article-body',\n",
    "            '.post-content',\n",
    "            '.entry-content',\n",
    "            '.story-content',\n",
    "            '.content',\n",
    "            '.body',\n",
    "            'main',\n",
    "            '[itemprop=\"articleBody\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            elements = soup.select(selector)\n",
    "            for element in elements:\n",
    "                # Clean up the text\n",
    "                text = element.get_text()\n",
    "                text = re.sub(r'\\s+', ' ', text).strip()\n",
    "                \n",
    "                if len(text) > 100:  # Reasonable minimum length for an article\n",
    "                    return text\n",
    "        \n",
    "        # If no specific content area found, try to find paragraphs with most text\n",
    "        all_paragraphs = soup.find_all('p')\n",
    "        if all_paragraphs:\n",
    "            # Find the longest continuous block of paragraphs\n",
    "            paragraphs_text = []\n",
    "            current_block = []\n",
    "            \n",
    "            for p in all_paragraphs:\n",
    "                p_text = p.get_text().strip()\n",
    "                if len(p_text) > 20:  # Minimum paragraph length\n",
    "                    current_block.append(p_text)\n",
    "                else:\n",
    "                    if len(current_block) > 3:  # Minimum block size\n",
    "                        paragraphs_text.append(' '.join(current_block))\n",
    "                    current_block = []\n",
    "            \n",
    "            if current_block and len(current_block) > 3:\n",
    "                paragraphs_text.append(' '.join(current_block))\n",
    "            \n",
    "            if paragraphs_text:\n",
    "                # Return the longest block\n",
    "                longest_block = max(paragraphs_text, key=len)\n",
    "                if len(longest_block) > 100:\n",
    "                    return longest_block\n",
    "        \n",
    "        return \"Body content not found or too short\"\n",
    "    \n",
    "    def extract_images(self, soup, base_url):\n",
    "        \"\"\"Extract images from the article\"\"\"\n",
    "        images = []\n",
    "        \n",
    "        # Look for images in various locations\n",
    "        img_selectors = [\n",
    "            'img',\n",
    "            'meta[property=\"og:image\"]',\n",
    "            'meta[name=\"twitter:image\"]',\n",
    "            'link[rel=\"image_src\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in img_selectors:\n",
    "            elements = soup.select(selector)\n",
    "            for element in elements:\n",
    "                if selector.startswith('meta'):\n",
    "                    img_url = element.get('content', '')\n",
    "                elif selector.startswith('link'):\n",
    "                    img_url = element.get('href', '')\n",
    "                else:\n",
    "                    img_url = element.get('src', '') or element.get('data-src', '')\n",
    "                \n",
    "                if img_url:\n",
    "                    # Make URL absolute\n",
    "                    img_url = urljoin(base_url, img_url)\n",
    "                    \n",
    "                    # Get alt text for regular img tags\n",
    "                    alt_text = element.get('alt', '') if selector == 'img' else ''\n",
    "                    \n",
    "                    # Avoid duplicates\n",
    "                    if img_url not in [img['url'] for img in images]:\n",
    "                        images.append({\n",
    "                            'url': img_url,\n",
    "                            'alt': alt_text\n",
    "                        })\n",
    "        \n",
    "        return images\n",
    "    \n",
    "    def extract_metadata(self, soup):\n",
    "        \"\"\"Extract additional metadata from the article\"\"\"\n",
    "        metadata = {}\n",
    "        \n",
    "        # Publication date\n",
    "        date_selectors = [\n",
    "            'meta[property=\"article:published_time\"]',\n",
    "            'meta[name=\"date\"]',\n",
    "            'meta[name=\"publish_date\"]',\n",
    "            'time',\n",
    "            '[class*=\"date\"]',\n",
    "            '[class*=\"time\"]',\n",
    "            '[itemprop=\"datePublished\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in date_selectors:\n",
    "            elements = soup.select(selector)\n",
    "            for element in elements:\n",
    "                if selector.startswith('meta'):\n",
    "                    date = element.get('content', '')\n",
    "                else:\n",
    "                    date = element.get('datetime', '') or element.get_text()\n",
    "                \n",
    "                if date and 'date' not in metadata:\n",
    "                    metadata['date'] = date.strip()\n",
    "                    break\n",
    "        \n",
    "        # Author\n",
    "        author_selectors = [\n",
    "            'meta[name=\"author\"]',\n",
    "            'meta[property=\"article:author\"]',\n",
    "            '[rel=\"author\"]',\n",
    "            '.author',\n",
    "            '[class*=\"author\"]',\n",
    "            '[class*=\"byline\"]',\n",
    "            '[itemprop=\"author\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in author_selectors:\n",
    "            elements = soup.select(selector)\n",
    "            for element in elements:\n",
    "                if selector.startswith('meta'):\n",
    "                    author = element.get('content', '')\n",
    "                else:\n",
    "                    author = element.get_text()\n",
    "                \n",
    "                if author and 'author' not in metadata:\n",
    "                    metadata['author'] = author.strip()\n",
    "                    break\n",
    "        \n",
    "        # Description\n",
    "        desc_selectors = [\n",
    "            'meta[property=\"og:description\"]',\n",
    "            'meta[name=\"description\"]',\n",
    "            'meta[name=\"twitter:description\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in desc_selectors:\n",
    "            elements = soup.select(selector)\n",
    "            for element in elements:\n",
    "                description = element.get('content', '')\n",
    "                if description and 'description' not in metadata:\n",
    "                    metadata['description'] = description.strip()\n",
    "                    break\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    def download_image(self, image_url, save_dir=None):\n",
    "        \"\"\"Download an image and return information about it\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(image_url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            img_data = response.content\n",
    "            img = Image.open(io.BytesIO(img_data))\n",
    "            \n",
    "            image_info = {\n",
    "                'url': image_url,\n",
    "                'format': img.format,\n",
    "                'size': len(img_data),\n",
    "                'dimensions': img.size,\n",
    "                'mode': img.mode,\n",
    "                'downloaded': True\n",
    "            }\n",
    "            \n",
    "            # Save image if directory provided\n",
    "            if save_dir:\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                filename = os.path.join(save_dir, f\"image_{int(time.time())}_{hash(image_url)}.{img.format.lower()}\")\n",
    "                with open(filename, 'wb') as f:\n",
    "                    f.write(img_data)\n",
    "                image_info['saved_path'] = filename\n",
    "            \n",
    "            return image_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading image {image_url}: {e}\")\n",
    "            return {\n",
    "                'url': image_url,\n",
    "                'error': str(e),\n",
    "                'downloaded': False\n",
    "            }\n",
    "    \n",
    "    def extract_article(self, url, download_images=False, image_save_dir=None):\n",
    "        \"\"\"\n",
    "        Extract information from a news article URL\n",
    "        \n",
    "        Args:\n",
    "            url (str): The URL of the news article\n",
    "            download_images (bool): Whether to download images\n",
    "            image_save_dir (str): Directory to save downloaded images\n",
    "            \n",
    "        Returns:\n",
    "            dict: Article information including header, body, and images\n",
    "        \"\"\"\n",
    "        if not self.is_valid_url(url):\n",
    "            return {\"error\": \"Invalid URL\"}\n",
    "        \n",
    "        print(f\"Extracting article from: {url}\")\n",
    "        \n",
    "        # Fetch the page content\n",
    "        content = self.get_page_content(url)\n",
    "        if not content:\n",
    "            return {\"error\": \"Failed to fetch page content\"}\n",
    "        \n",
    "        # Parse with BeautifulSoup\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        \n",
    "        # Extract article components\n",
    "        title = self.extract_title(soup)\n",
    "        body = self.extract_body(soup)\n",
    "        images = self.extract_images(soup, url)\n",
    "        metadata = self.extract_metadata(soup)\n",
    "        \n",
    "        # Download images if requested\n",
    "        downloaded_images = []\n",
    "        if download_images and images:\n",
    "            print(f\"Downloading {len(images)} images...\")\n",
    "            for img in images:\n",
    "                img_info = self.download_image(img['url'], image_save_dir)\n",
    "                img_info['alt'] = img.get('alt', '')\n",
    "                downloaded_images.append(img_info)\n",
    "        \n",
    "        # Prepare result\n",
    "        result = {\n",
    "            \"url\": url,\n",
    "            \"title\": title,\n",
    "            \"body\": body,\n",
    "            \"images\": downloaded_images if download_images else images,\n",
    "            \"metadata\": metadata,\n",
    "            \"success\": True\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def process_user_input(self):\n",
    "        \"\"\"Interactive mode for processing URLs\"\"\"\n",
    "        print(\"Enhanced News Article Extraction Agent\")\n",
    "        print(\"Enter news article URLs to extract content (or type 'quit' to exit):\")\n",
    "        \n",
    "        while True:\n",
    "            url = input(\"\\nEnter URL: \").strip()\n",
    "            \n",
    "            if url.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"Exiting News Article Extractor.\")\n",
    "                break\n",
    "                \n",
    "            if not url:\n",
    "                print(\"Please enter a URL.\")\n",
    "                continue\n",
    "                \n",
    "            if not self.is_valid_url(url):\n",
    "                print(\"Invalid URL. Please enter a valid URL including http:// or https://\")\n",
    "                continue\n",
    "            \n",
    "            # Ask if user wants to download images\n",
    "            download_choice = input(\"Download images? (y/n): \").strip().lower()\n",
    "            download_images = download_choice in ['y', 'yes']\n",
    "            \n",
    "            image_save_dir = None\n",
    "            if download_images:\n",
    "                image_save_dir = input(\"Enter directory to save images (press Enter for current directory): \").strip()\n",
    "                if not image_save_dir:\n",
    "                    image_save_dir = \"downloaded_images\"\n",
    "            \n",
    "            # Extract article\n",
    "            print(f\"\\nProcessing {url}...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            result = self.extract_article(url, download_images, image_save_dir)\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            print(f\"Processing completed in {processing_time:.2f} seconds\")\n",
    "            \n",
    "            # Display results\n",
    "            if result.get('error'):\n",
    "                print(f\"Error: {result['error']}\")\n",
    "                continue\n",
    "                \n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(f\"TITLE: {result['title']}\")\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "            if result['metadata']:\n",
    "                print(\"\\nMETADATA:\")\n",
    "                for key, value in result['metadata'].items():\n",
    "                    print(f\"  {key.capitalize()}: {value}\")\n",
    "            \n",
    "            print(f\"\\nBODY (first 500 characters):\")\n",
    "            body_preview = result['body'][:500] + \"...\" if len(result['body']) > 500 else result['body']\n",
    "            print(f\"{body_preview}\")\n",
    "            \n",
    "            print(f\"\\nIMAGES FOUND: {len(result['images'])}\")\n",
    "            for i, img in enumerate(result['images'][:5], 1):  # Show first 5 images\n",
    "                print(f\"  {i}. {img['url']}\")\n",
    "                if img.get('alt'):\n",
    "                    print(f\"     Alt text: {img['alt']}\")\n",
    "                if img.get('downloaded', False) and img['downloaded']:\n",
    "                    print(f\"     Downloaded: {img.get('saved_path', 'Yes')}\")\n",
    "                elif img.get('error'):\n",
    "                    print(f\"     Error: {img['error']}\")\n",
    "            \n",
    "            if len(result['images']) > 5:\n",
    "                print(f\"  ... and {len(result['images']) - 5} more images\")\n",
    "            \n",
    "            # Ask if user wants to save results to file\n",
    "            save_choice = input(\"\\nSave results to JSON file? (y/n): \").strip().lower()\n",
    "            if save_choice in ['y', 'yes']:\n",
    "                filename = f\"article_{int(time.time())}.json\"\n",
    "                with open(filename, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "                print(f\"Results saved to {filename}\")\n",
    "\n",
    "# Alternative approach using RSS feeds or APIs for difficult sites\n",
    "def try_alternative_sources(url):\n",
    "    \"\"\"Try alternative methods for difficult-to-scrape sites\"\"\"\n",
    "    print(f\"Trying alternative methods for: {url}\")\n",
    "    \n",
    "    # For Mothership.sg, we can try their RSS feed\n",
    "    if \"mothership.sg\" in url:\n",
    "        rss_url = \"https://mothership.sg/feed/\"\n",
    "        try:\n",
    "            response = requests.get(rss_url, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'xml')\n",
    "                items = soup.find_all('item')\n",
    "                \n",
    "                # Try to find the article in the RSS feed\n",
    "                for item in items:\n",
    "                    if url in item.find('link').text:\n",
    "                        title = item.find('title').text if item.find('title') else \"Title not found\"\n",
    "                        description = item.find('description').text if item.find('description') else \"Description not found\"\n",
    "                        \n",
    "                        return {\n",
    "                            \"url\": url,\n",
    "                            \"title\": title,\n",
    "                            \"body\": description,\n",
    "                            \"images\": [],\n",
    "                            \"metadata\": {},\n",
    "                            \"source\": \"RSS feed\",\n",
    "                            \"success\": True\n",
    "                        }\n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing RSS feed: {e}\")\n",
    "    \n",
    "    return {\"error\": \"Could not extract content using alternative methods\"}\n",
    "\n",
    "# Demonstration and Testing\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the enhanced extractor\n",
    "    extractor = EnhancedNewsArticleExtractor()\n",
    "    \n",
    "    # Test with the problematic URL\n",
    "    test_url = \"https://mothership.sg/2025/07/soviet-era-plane/\"\n",
    "    print(f\"Testing with: {test_url}\")\n",
    "    \n",
    "    # Try direct extraction first\n",
    "    result = extractor.extract_article(test_url)\n",
    "    \n",
    "    if result.get('error'):\n",
    "        print(\"Direct extraction failed, trying alternative methods...\")\n",
    "        result = try_alternative_sources(test_url)\n",
    "    \n",
    "    if result.get('success'):\n",
    "        print(f\"Successfully extracted: {result['title']}\")\n",
    "        print(f\"Body preview: {result['body'][:200]}...\")\n",
    "    else:\n",
    "        print(f\"Failed to extract content: {result.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    # Start interactive mode\n",
    "    extractor.process_user_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "228350a6-f241-49f5-bef0-8429453c172f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./anaconda3/envs/myenv/lib/python3.10/site-packages (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4 in ./anaconda3/envs/myenv/lib/python3.10/site-packages (4.12.2)\n",
      "Requirement already satisfied: fake-useragent in ./anaconda3/envs/myenv/lib/python3.10/site-packages (2.2.0)\n",
      "Requirement already satisfied: pillow in ./anaconda3/envs/myenv/lib/python3.10/site-packages (10.2.0)\n",
      "Collecting cloudscraper\n",
      "  Downloading cloudscraper-1.2.71-py2.py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./anaconda3/envs/myenv/lib/python3.10/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/envs/myenv/lib/python3.10/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./anaconda3/envs/myenv/lib/python3.10/site-packages (from requests) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/envs/myenv/lib/python3.10/site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./anaconda3/envs/myenv/lib/python3.10/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: pyparsing>=2.4.7 in ./anaconda3/envs/myenv/lib/python3.10/site-packages (from cloudscraper) (3.0.9)\n",
      "Collecting requests-toolbelt>=0.9.1 (from cloudscraper)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Downloading cloudscraper-1.2.71-py2.py3-none-any.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.7/99.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Installing collected packages: requests-toolbelt, cloudscraper\n",
      "Successfully installed cloudscraper-1.2.71 requests-toolbelt-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 fake-useragent pillow cloudscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82036a1-f9e9-42d5-8404-7b0638624031",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-myenv]",
   "language": "python",
   "name": "conda-env-anaconda3-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
